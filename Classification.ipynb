{
    "nbformat": 4, 
    "cells": [
        {
            "cell_type": "markdown", 
            "metadata": {}, 
            "source": "## PySpark Classification\n#### Reference Libraries\n\n1. [StringIndexer](#stringindexer)\n2. [OneHotEncoder](#onehotencoder)\n3. [VectorAssembler](#vectorassembler)\n4. [Pipeline](#pipeline)\n5. [BinaryClassificationEvaluator & MulticlassClassificationEvaluator](#classificationevaluator)\n6. [ParamGrid & CrossValidator](#paramgrid&crossvalidator)\n7. [StandardScaler](#stdscaler)\n\nGo to [Main Scripts](#maincode)"
        }, 
        {
            "cell_type": "markdown", 
            "metadata": {}, 
            "source": "<a id=\"stringindexer\"></a>\n#### 1. StringIndexer\nEncode a **string column of labels** to a column of label indices. \n- The indices are in [0, numLabels), ordered by label frequencies, so the most frequent label gets index 0. \n- If the input column is numeric, we cast it to string and index the string values.\n- There are two strategies to handle unseen labels when you have fit on one dataset and use it to transform another:\n  - throw an exception (which is the default)\n  - skip the row containing the unseen label entirely -> setHandleInvalid(\"skip\")"
        }, 
        {
            "cell_type": "markdown", 
            "metadata": {}, 
            "source": "<a id=\"onehotencoder\"></a>\n#### 2. OneHotEncoder\nMap a column of label indices to a column of binary vectors, with at most a single one-value. \n- **Input column must be Numeric** (use StringIndexer prior to OneHotEncoder if the categorical is labelled as string)\n- This encoding allows algorithms which expect continuous features, such as Logistic Regression, to use categorical features.\n- Return as a sparse vector SparseVector(vector_size, [nonzero_indices], [nonzero_values]}) e.g. SparseVector(3, [1, 2], [1.0, 1.0]) => [1.0 0.0 1.0]"
        }, 
        {
            "cell_type": "code", 
            "metadata": {
                "collapsed": true
            }, 
            "source": "from pyspark.ml.feature import OneHotEncoder, StringIndexer\n\ndf = spark.createDataFrame([\n    (0, \"a\"),\n    (1, \"b\"),\n    (2, \"c\"),\n    (3, \"a\"),\n    (4, \"a\"),\n    (5, \"c\"),\n    (6, \"d\")\n], [\"id\", \"category\"])\n\nstringIndexer = StringIndexer(inputCol=\"category\", outputCol=\"categoryIndex\")\nmodel = stringIndexer.fit(df)\nindexed = model.transform(df)\n\nencoder = OneHotEncoder(inputCol=\"categoryIndex\", outputCol=\"categoryVec\")\nencoded = encoder.transform(indexed)\nencoded.show()", 
            "outputs": [], 
            "execution_count": null
        }, 
        {
            "cell_type": "markdown", 
            "metadata": {}, 
            "source": "<a id=\"vectorassembler\"></a>\n#### 3. VectorAssembler\nCombine a list of columns into a single vector column. It is useful to combine raw features and features generated by different transformers into a single feature vector, to train ML models (e.g. logistic regression and decision trees)\n- In each row, the values of the input columns will be concatenated into a vector (in the specified order)\n- Accepts input types: **numeric, boolean, vector**\n- only \"fit\", no \"transform\""
        }, 
        {
            "cell_type": "code", 
            "metadata": {
                "collapsed": true
            }, 
            "source": "from pyspark.ml.feature import VectorAssembler\n\nassembler = VectorAssembler(inputCols=[\"categoryIndex\", \"categoryVec\"], outputCol=\"features\")\noutput = assembler.transform(encoded)\noutput.show()", 
            "outputs": [], 
            "execution_count": null
        }, 
        {
            "cell_type": "markdown", 
            "metadata": {}, 
            "source": "<a id=\"pipeline\"></a>\n#### 4. Pipeline\nTie multiple stages of ML tasks together (e.g. feature transformations) in order to simplify the code"
        }, 
        {
            "cell_type": "code", 
            "metadata": {
                "collapsed": true
            }, 
            "source": "from pyspark.ml import Pipeline\nfrom pyspark.ml.feature import OneHotEncoder, StringIndexer, VectorAssembler\n\ndf = spark.createDataFrame([\n    (0, \"a\"),\n    (1, \"b\"),\n    (2, \"c\"),\n    (3, \"a\"),\n    (4, \"a\"),\n    (5, \"c\"),\n    (6, \"d\")\n], [\"id\", \"category\"])\n\nstages = [] # stages in our Pipeline\nstringIndexer = StringIndexer(inputCol=\"category\", outputCol=\"categoryIndex\")\nencoder = OneHotEncoder(inputCol=\"categoryIndex\", outputCol=\"categoryVec\")\nassembler = VectorAssembler(inputCols=[\"categoryIndex\", \"categoryVec\"], outputCol=\"features\")\nstages += [stringIndexer, encoder, assembler]\npipeline = Pipeline(stages=stages) # create a Pipeline\n\npipelineModel = pipeline.fit(df)\nencoded = pipelineModel.transform(df)\nencoded.show()", 
            "outputs": [], 
            "execution_count": null
        }, 
        {
            "cell_type": "markdown", 
            "metadata": {}, 
            "source": "<a id=\"classificationevaluator\"></a>\n#### 5. BinaryClassificationEvaluator & MultiClassificationEvaluator\nEvaluator for binary classification, which expects two input columns: **rawPrediction and label** \n- Column \"rawPrediction\" can be double (binary 0/1 prediction, or probability of label 1) or of type vector (length-2 vector of raw predictions, scores, or label probabilities)."
        }, 
        {
            "cell_type": "markdown", 
            "metadata": {}, 
            "source": "<a id=\"paramgrid&crossvalidator\"></a>\n#### 6. ParamGrid & CrossValidator\n"
        }, 
        {
            "cell_type": "code", 
            "metadata": {
                "collapsed": true
            }, 
            "source": "from pyspark.mllib.evaluation import MultilabelMetrics\npredictionAndLabels = sc.parallelize([([0.0, 1.0], [0.0, 2.0]), ([0.0, 2.0], [0.0, 1.0]), \n                                      ([], [0.0]), ([2.0], [2.0]), ([2.0, 0.0], [2.0, 0.0]), \n                                      ([0.0, 1.0, 2.0], [0.0, 1.0]), ([1.0], [1.0, 2.0])])\n\nprint predictionAndLabels.take(2)\nprint type(predictionAndLabels)\nmetrics = MultilabelMetrics(predictionAndLabels)\n\nprint metrics.precision(0.0)\nprint metrics.recall(1.0)\nprint metrics.f1Measure(2.0)\nprint metrics.precision()\nprint metrics.recall()\nprint metrics.f1Measure()\nprint metrics.microPrecision\nprint metrics.microRecall\nprint metrics.microF1Measure\nprint metrics.hammingLoss\nprint metrics.subsetAccuracy\nprint metrics.accuracy", 
            "outputs": [], 
            "execution_count": null
        }, 
        {
            "cell_type": "code", 
            "metadata": {
                "collapsed": true
            }, 
            "source": "from pyspark.ml.feature import StandardScaler, MinMaxScaler \nfrom pyspark.ml.linalg import Vectors\n# from pyspark.sql.functions import col, round\n\ndf = spark.createDataFrame([(Vectors.dense([-1.0, 2.5, 0.0]),),\\\n                            (Vectors.dense([2.0, 0.0, 1.5]),),\\\n                            (Vectors.dense([3.0, 1.0, -0.5]),)],\\\n                           ['features'])\n\nscaler_minmax = MinMaxScaler(inputCol='features', outputCol='scaledMinmax_feature')\nscalerModel = scaler_minmax.fit(df)\nscaledData = scalerModel.transform(df)\nscaledData.show()\n\nscaler_std = StandardScaler(inputCol='features', outputCol=\"scaledStd_feature\", withStd=True, withMean=False)\nscalerModel = scaler_std.fit(df)\nscaledData = scalerModel.transform(df)\nscaledData.show()\n\n# spark.createDataFrame([(2.546,)], ['a']).select(round('a', 1).alias('r')).collect()\n# scaledData.select(round('scaledStd_feature', 1).alias('r')).collect()\n# scaledData.rdd.map(lambda x: map(lambda y: round(y,4), x))", 
            "outputs": [], 
            "execution_count": null
        }, 
        {
            "cell_type": "markdown", 
            "metadata": {}, 
            "source": "<a id=\"maincode\"></a>\n#### Main Scripts"
        }, 
        {
            "cell_type": "code", 
            "metadata": {
                "collapsed": true
            }, 
            "source": "import pandas as pd\nimport numpy as np\n\nfrom pyspark.sql import Row\n\nfrom sklearn import datasets\nfrom pyspark.ml import Pipeline\nfrom pyspark.ml.feature import OneHotEncoder, StringIndexer, VectorAssembler ###One-Hot Encoding\n\nfrom pyspark.mllib.regression import LabeledPoint\nfrom pyspark.mllib.tree import RandomForest", 
            "outputs": [], 
            "execution_count": 4
        }, 
        {
            "cell_type": "code", 
            "metadata": {}, 
            "source": "iris = datasets.load_iris()\nX = iris.data\nY = iris.target\n\niris_rdd = sc.parallelize(np.c_[X,Y]).map(lambda x: Row(sepal_length=float(x[0]),\\\n                                                        sepal_width=float(x[1]),\\\n                                                        petal_length=float(x[2]),\\\n                                                        petal_width=float(x[3]),\\\n                                                        target=str(x[4])))\niris_rddDF = spark.createDataFrame(iris_rdd)\ncols = iris_rddDF.columns\nprint 'Columns:', cols\nprint ''\niris_rddDF.show(3)", 
            "outputs": [
                {
                    "text": "Columns: ['petal_length', 'petal_width', 'sepal_length', 'sepal_width', 'target']\n\n+------------+-----------+------------+-----------+------+\n|petal_length|petal_width|sepal_length|sepal_width|target|\n+------------+-----------+------------+-----------+------+\n|         1.4|        0.2|         5.1|        3.5|   0.0|\n|         1.4|        0.2|         4.9|        3.0|   0.0|\n|         1.3|        0.2|         4.7|        3.2|   0.0|\n+------------+-----------+------------+-----------+------+\nonly showing top 3 rows\n\n", 
                    "output_type": "stream", 
                    "name": "stdout"
                }
            ], 
            "execution_count": 5
        }, 
        {
            "cell_type": "code", 
            "metadata": {
                "collapsed": true
            }, 
            "source": "###One-Hot Encoding\ncategoricalColumns = []\nstages = [] # stages in our Pipeline\nfor categoricalCol in categoricalColumns:\n  # Category Indexing with StringIndexer\n  stringIndexer = StringIndexer(inputCol=categoricalCol, outputCol=categoricalCol+\"Index\")\n  # Use OneHotEncoder to convert categorical variables into binary SparseVectors\n  encoder = OneHotEncoder(inputCol=categoricalCol+\"Index\", outputCol=categoricalCol+\"classVec\")\n  # Add stages.  These are not run here, but will run all at once later on.\n  stages += [stringIndexer, encoder]", 
            "outputs": [], 
            "execution_count": 7
        }, 
        {
            "cell_type": "code", 
            "metadata": {
                "collapsed": true
            }, 
            "source": "# Convert label into label indices using the StringIndexer\nlabel_stringIdx = StringIndexer(inputCol = \"target\", outputCol = \"label\")\nstages += [label_stringIdx]", 
            "outputs": [], 
            "execution_count": 8
        }, 
        {
            "cell_type": "code", 
            "metadata": {
                "collapsed": true
            }, 
            "source": "# Transform all features into a vector using VectorAssembler\nnumericCols = [\"sepal_length\", \"sepal_width\", \"petal_length\", \"petal_width\"]\nassemblerInputs = map(lambda c: c + \"classVec\", categoricalColumns) + numericCols\nassembler = VectorAssembler(inputCols=assemblerInputs, outputCol=\"features\")\nstages += [assembler]", 
            "outputs": [], 
            "execution_count": 9
        }, 
        {
            "cell_type": "code", 
            "metadata": {
                "scrolled": false
            }, 
            "source": "# Create a Pipeline.\npipeline = Pipeline(stages=stages)\n# Run the feature transformations.\n#  - fit() computes feature statistics as needed.\n#  - transform() actually transforms the features.\npipelineModel = pipeline.fit(iris_rddDF)\niris_rddDF = pipelineModel.transform(iris_rddDF)\n\n# Keep relevant columns\nselectedcols = [\"label\", \"features\"] + cols\niris_rddDF = iris_rddDF.select(selectedcols)\niris_rddDF.show()", 
            "outputs": [
                {
                    "text": "+-----+-----------------+------------+-----------+------------+-----------+------+\n|label|         features|petal_length|petal_width|sepal_length|sepal_width|target|\n+-----+-----------------+------------+-----------+------------+-----------+------+\n|  0.0|[5.1,3.5,1.4,0.2]|         1.4|        0.2|         5.1|        3.5|   0.0|\n|  0.0|[4.9,3.0,1.4,0.2]|         1.4|        0.2|         4.9|        3.0|   0.0|\n|  0.0|[4.7,3.2,1.3,0.2]|         1.3|        0.2|         4.7|        3.2|   0.0|\n|  0.0|[4.6,3.1,1.5,0.2]|         1.5|        0.2|         4.6|        3.1|   0.0|\n|  0.0|[5.0,3.6,1.4,0.2]|         1.4|        0.2|         5.0|        3.6|   0.0|\n|  0.0|[5.4,3.9,1.7,0.4]|         1.7|        0.4|         5.4|        3.9|   0.0|\n|  0.0|[4.6,3.4,1.4,0.3]|         1.4|        0.3|         4.6|        3.4|   0.0|\n|  0.0|[5.0,3.4,1.5,0.2]|         1.5|        0.2|         5.0|        3.4|   0.0|\n|  0.0|[4.4,2.9,1.4,0.2]|         1.4|        0.2|         4.4|        2.9|   0.0|\n|  0.0|[4.9,3.1,1.5,0.1]|         1.5|        0.1|         4.9|        3.1|   0.0|\n|  0.0|[5.4,3.7,1.5,0.2]|         1.5|        0.2|         5.4|        3.7|   0.0|\n|  0.0|[4.8,3.4,1.6,0.2]|         1.6|        0.2|         4.8|        3.4|   0.0|\n|  0.0|[4.8,3.0,1.4,0.1]|         1.4|        0.1|         4.8|        3.0|   0.0|\n|  0.0|[4.3,3.0,1.1,0.1]|         1.1|        0.1|         4.3|        3.0|   0.0|\n|  0.0|[5.8,4.0,1.2,0.2]|         1.2|        0.2|         5.8|        4.0|   0.0|\n|  0.0|[5.7,4.4,1.5,0.4]|         1.5|        0.4|         5.7|        4.4|   0.0|\n|  0.0|[5.4,3.9,1.3,0.4]|         1.3|        0.4|         5.4|        3.9|   0.0|\n|  0.0|[5.1,3.5,1.4,0.3]|         1.4|        0.3|         5.1|        3.5|   0.0|\n|  0.0|[5.7,3.8,1.7,0.3]|         1.7|        0.3|         5.7|        3.8|   0.0|\n|  0.0|[5.1,3.8,1.5,0.3]|         1.5|        0.3|         5.1|        3.8|   0.0|\n+-----+-----------------+------------+-----------+------------+-----------+------+\nonly showing top 20 rows\n\n", 
                    "output_type": "stream", 
                    "name": "stdout"
                }
            ], 
            "execution_count": 10
        }, 
        {
            "cell_type": "code", 
            "metadata": {
                "collapsed": true
            }, 
            "source": "print type(iris_rddDF)", 
            "outputs": [], 
            "execution_count": null
        }, 
        {
            "cell_type": "code", 
            "metadata": {
                "collapsed": true
            }, 
            "source": "### Randomly split data into training and test sets. set seed for reproducibility\n(trainingData, validateData, testData) = iris_rddDF.randomSplit([0.5, 0.1, 0.4], seed=100)\nprint trainingData.count()\nprint validateData.count()\nprint testData.count()", 
            "outputs": [], 
            "execution_count": null
        }, 
        {
            "cell_type": "code", 
            "metadata": {
                "collapsed": true
            }, 
            "source": "from pyspark.ml.classification import RandomForestClassifier\n\n# Create an initial RandomForest model.\nrf = RandomForestClassifier(labelCol=\"label\", featuresCol=\"features\", numTrees=2)\n\n# Train model with Training Data\nrfModel = rf.fit(trainingData)", 
            "outputs": [], 
            "execution_count": null
        }, 
        {
            "cell_type": "code", 
            "metadata": {
                "collapsed": true
            }, 
            "source": "# Make predictions on test data using the Transformer.transform() method.\npredictions = rfModel.transform(testData)", 
            "outputs": [], 
            "execution_count": null
        }, 
        {
            "cell_type": "code", 
            "metadata": {
                "collapsed": true
            }, 
            "source": "predictions.printSchema()\n\n#\"rawPrediction\": Vector of length classes, with the counts of training instance labels at the tree node which makes the prediction\n#\"probability\": Vector of length # classes equal to rawPrediction normalized to a multinomial distribution", 
            "outputs": [], 
            "execution_count": null
        }, 
        {
            "cell_type": "code", 
            "metadata": {
                "collapsed": true
            }, 
            "source": "selected = predictions.select(\"label\", \"prediction\", \"probability\", \"rawPrediction\", \"features\")\nselected.show()", 
            "outputs": [], 
            "execution_count": null
        }, 
        {
            "cell_type": "code", 
            "metadata": {
                "collapsed": true
            }, 
            "source": "from pyspark.ml.evaluation import MulticlassClassificationEvaluator\n\n# Evaluate model\nevaluator = MulticlassClassificationEvaluator()\nprint evaluator.evaluate(predictions)\n\nfrom pyspark.mllib.evaluation import MultilabelMetrics\nresult = predictions.select(['prediction','label']).rdd\n# print result.map(lambda x: (x.prediction, x.label)).take(5)\nmetrics = MultilabelMetrics(result.map(lambda x: ([x.prediction], [x.label])))\n\n# Summary stats\nprint(\"Recall = %s\" % metrics.recall())\nprint(\"Precision = %s\" % metrics.precision())\nprint(\"F1 measure = %s\" % metrics.f1Measure())\nprint(\"Accuracy = %s\" % metrics.accuracy)", 
            "outputs": [], 
            "execution_count": null
        }, 
        {
            "cell_type": "code", 
            "metadata": {
                "collapsed": true
            }, 
            "source": "# Create ParamGrid for Cross Validation\nfrom pyspark.ml.tuning import ParamGridBuilder, CrossValidator\n\nparamGrid = (ParamGridBuilder()\n             .addGrid(rf.maxDepth, [2, 4, 6])\n             .addGrid(rf.maxBins, [20, 60])\n             .addGrid(rf.numTrees, [5, 20])\n             .build())\n# 3 values for maxDepth, 2 values for maxBin, and 2 values for numTrees. \n# This grid will have 3 x 2 x 2 = 12 parameter settings for CrossValidator to choose from. ", 
            "outputs": [], 
            "execution_count": null
        }, 
        {
            "cell_type": "code", 
            "metadata": {
                "collapsed": true
            }, 
            "source": "# Create 5-fold CrossValidator\ncv = CrossValidator(estimator=rf, estimatorParamMaps=paramGrid, evaluator=evaluator, numFolds=5)\n\n# Run cross validations.  This can take about 6 minutes since it is training over 20 trees!\ncvModel = cv.fit(trainingData)", 
            "outputs": [], 
            "execution_count": null
        }, 
        {
            "cell_type": "code", 
            "metadata": {
                "collapsed": true
            }, 
            "source": "# Use test set here so we can measure the accuracy of our model on new data\npredictions = cvModel.transform(validateData)", 
            "outputs": [], 
            "execution_count": null
        }, 
        {
            "cell_type": "code", 
            "metadata": {
                "collapsed": true
            }, 
            "source": "# cvModel uses the best model found from the Cross Validation\n# Evaluate best model\nevaluator.evaluate(predictions)", 
            "outputs": [], 
            "execution_count": null
        }, 
        {
            "cell_type": "code", 
            "metadata": {
                "collapsed": true
            }, 
            "source": "# View Best model's predictions and probabilities of each prediction class\nselected = predictions.select(\"label\", \"prediction\", \"probability\", \"rawPrediction\", \"features\")\nselected.show()", 
            "outputs": [], 
            "execution_count": null
        }, 
        {
            "cell_type": "code", 
            "metadata": {
                "collapsed": true
            }, 
            "source": "# View Best model's parameter settings\nbestModel = cvModel.bestModel\nprint 'numTrees:', bestModel.getNumTrees\nprint 'Tree Weights:', bestModel.treeWeights\nprint 'Feature Importances:', bestModel.featureImportances\nprint 'Total numNodes:', bestModel.totalNumNodes\nprint bestModel.params\n# print 'Trees:', bestModel.trees\n# print 'Description:', bestModel.toDebugString", 
            "outputs": [], 
            "execution_count": null
        }, 
        {
            "cell_type": "code", 
            "metadata": {
                "collapsed": true
            }, 
            "source": "# Generate predictions for entire dataset\nfinalPredictions = bestModel.transform(testData)", 
            "outputs": [], 
            "execution_count": null
        }, 
        {
            "cell_type": "code", 
            "metadata": {
                "collapsed": true
            }, 
            "source": "# Evaluate best model\nevaluator.evaluate(finalPredictions)", 
            "outputs": [], 
            "execution_count": null
        }, 
        {
            "cell_type": "code", 
            "metadata": {
                "collapsed": true
            }, 
            "source": "test = pd.concat([finalPredictions.select('prediction').toPandas(), finalPredictions.select('label').toPandas()], axis=1)\nany(test['prediction'] - test ['label'])", 
            "outputs": [], 
            "execution_count": null
        }, 
        {
            "cell_type": "markdown", 
            "metadata": {}, 
            "source": "### Others"
        }, 
        {
            "cell_type": "code", 
            "metadata": {
                "collapsed": true
            }, 
            "source": "test1 = sc.parallelize([(0.0, 1.0, 3.0), (0.0, 1.0, 4.0), (0.0, 2.0, 5.0)])\nprint test1.take(1)\nprint test1.map(lambda x: ((x[0], x[1]), x[2])).distinct().take(10)", 
            "outputs": [], 
            "execution_count": null
        }, 
        {
            "cell_type": "code", 
            "metadata": {}, 
            "source": "from pyspark.mllib.stat import Statistics\nimport numpy as np\ndata = sc.parallelize(\n    [np.array([1.0, np.nan]), np.array([2.0, 20.0]), np.array([5.0, 33.0]), np.array([5.0, 33.0])]\n)  # an RDD of Vectors\n\n# calculate the correlation matrix using Pearson's method. Use \"spearman\" for Spearman's method.\n# If a method is not specified, Pearson's method will be used by default.\nprint(Statistics.corr(data, method=\"pearson\"))", 
            "outputs": [
                {
                    "text": "[[  1.  nan]\n [ nan   1.]]\n", 
                    "output_type": "stream", 
                    "name": "stdout"
                }
            ], 
            "execution_count": 3
        }, 
        {
            "cell_type": "code", 
            "metadata": {
                "collapsed": true
            }, 
            "source": "", 
            "outputs": [], 
            "execution_count": null
        }
    ], 
    "metadata": {
        "kernelspec": {
            "display_name": "Python 2 with Spark 2.0", 
            "name": "python2-spark20", 
            "language": "python"
        }, 
        "language_info": {
            "codemirror_mode": {
                "version": 2, 
                "name": "ipython"
            }, 
            "version": "2.7.11", 
            "nbconvert_exporter": "python", 
            "mimetype": "text/x-python", 
            "file_extension": ".py", 
            "name": "python", 
            "pygments_lexer": "ipython2"
        }
    }, 
    "nbformat_minor": 1
}